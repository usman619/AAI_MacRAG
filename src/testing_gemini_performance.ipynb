{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0d817ba",
   "metadata": {},
   "source": [
    "# Testing Gemini Performance with Local Ollama Embeddings\n",
    "\n",
    "This notebook tests the MacRAG system using:\n",
    "- **Embeddings**: Local Ollama (`nomic-embed-text`)\n",
    "- **Generation**: Gemini API (`gemini-2.5-flash`)\n",
    "\n",
    "## Prerequisites\n",
    "1. Ollama running locally: `ollama serve`\n",
    "2. Pull embedding model: `ollama pull nomic-embed-text`\n",
    "3. Set `GEMINI_API_KEY` in `.env` file\n",
    "\n",
    "## ‚ö†Ô∏è Key Differences from Original Paper Results\n",
    "\n",
    "To match the original MacRAG paper results, you need to address these issues:\n",
    "\n",
    "### 1. **Proper Index Generation**\n",
    "The original uses `gen_index_macrag.py` with structured chunks containing metadata. Our simplified index uses plain text chunks which limits functionality.\n",
    "\n",
    "### 2. **Gemini Output Format**\n",
    "Gemini tends to give verbose responses like \"The answer is not in the passages\" instead of concise answers like \"Gates v. Collier\". This significantly reduces F1 scores.\n",
    "\n",
    "### 3. **Rate Limiting** \n",
    "Free Gemini tier only allows 10 requests/minute, making full evaluation slow (~25 minutes for 200 questions).\n",
    "\n",
    "### 4. **Recommended Settings for Better Results**\n",
    "```bash\n",
    "# Use prompt_version 3 for more concise answers\n",
    "--prompt_version 3\n",
    "\n",
    "# Enable reranking for better retrieval\n",
    "--with_reranking 1\n",
    "\n",
    "# Use the proper MacRAG index path\n",
    "--r_path processed/sum_600_400_raw_1500_500_e5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d41ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/usman619/python_code/MacRAG\n",
      "Ollama URL: http://localhost:11434\n",
      "Ollama model: nomic-embed-text\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'src' else Path.cwd()\n",
    "sys.path.insert(0, str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# Set Ollama environment variables\n",
    "os.environ['OLLAMA_BASE_URL'] = 'http://localhost:11434'\n",
    "os.environ['OLLAMA_EMBED_MODEL'] = 'nomic-embed-text'\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Ollama URL: {os.environ['OLLAMA_BASE_URL']}\")\n",
    "print(f\"Ollama model: {os.environ['OLLAMA_EMBED_MODEL']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c41dafb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY set: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (for GEMINI_API_KEY)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(PROJECT_ROOT / '.env')\n",
    "\n",
    "print(f\"GEMINI_API_KEY set: {bool(os.getenv('GEMINI_API_KEY'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36afdd25",
   "metadata": {},
   "source": [
    "## 1. Test Ollama Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6667465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ollama connection successful!\n",
      "   Model: nomic-embed-text\n",
      "   Embedding dimension: 768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def test_ollama_connection():\n",
    "    \"\"\"Test if Ollama is running and embedding model is available.\"\"\"\n",
    "    base_url = os.environ.get('OLLAMA_BASE_URL', 'http://localhost:11434')\n",
    "    model = os.environ.get('OLLAMA_EMBED_MODEL', 'nomic-embed-text')\n",
    "    \n",
    "    try:\n",
    "        # Test embedding\n",
    "        resp = requests.post(\n",
    "            f\"{base_url}/api/embed\",\n",
    "            json={\"model\": model, \"input\": [\"Hello, world!\"]},\n",
    "            timeout=30\n",
    "        )\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        emb = data.get('embeddings', [[]])[0]\n",
    "        print(f\"‚úÖ Ollama connection successful!\")\n",
    "        print(f\"   Model: {model}\")\n",
    "        print(f\"   Embedding dimension: {len(emb)}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Ollama connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "test_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe050109",
   "metadata": {},
   "source": [
    "## 2. Test Gemini API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e96dc217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/usman619/anaconda3/envs/pydev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini SDK installed: True\n",
      "Gemini API configured: True\n",
      "\n",
      "‚úÖ Gemini response: Hello\n",
      "\n",
      "‚úÖ Gemini response: Hello\n"
     ]
    }
   ],
   "source": [
    "from utils.gemini_handler import get_gemini_response, _HAS_GENAI, _CONFIGURED\n",
    "\n",
    "print(f\"Gemini SDK installed: {_HAS_GENAI}\")\n",
    "print(f\"Gemini API configured: {_CONFIGURED}\")\n",
    "\n",
    "if _HAS_GENAI and _CONFIGURED:\n",
    "    response = get_gemini_response(\"Say hello in one word.\", model_name=\"gemini-2.5-flash\", temperature=0)\n",
    "    print(f\"\\n‚úÖ Gemini response: {response}\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Gemini not configured. Check GEMINI_API_KEY in .env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4d382",
   "metadata": {},
   "source": [
    "## 3. Import Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d9a9c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions loaded\n"
     ]
    }
   ],
   "source": [
    "from metric import F1_scorer\n",
    "import numpy as np\n",
    "\n",
    "def replace_and_calculate_average(lst):\n",
    "    \"\"\"Replace -1 values with the average of valid values.\"\"\"\n",
    "    valid_values = [x for x in lst if x != -1]\n",
    "    if not valid_values:\n",
    "        return 0.0\n",
    "    average_of_valid_values = sum(valid_values) / len(valid_values)\n",
    "    replaced_list = [average_of_valid_values if x == -1 else x for x in lst]\n",
    "    return sum(replaced_list) / len(replaced_list)\n",
    "\n",
    "def eval_function(data, pred_dir, max_samples=200):\n",
    "    \"\"\"Evaluate predictions against ground truth.\"\"\"\n",
    "    answer_path = PROJECT_ROOT / \"data\" / \"eval\" / f\"{data}.json\"\n",
    "    \n",
    "    with open(answer_path, encoding='utf-8') as f:\n",
    "        qs_data = json.load(f)\n",
    "    \n",
    "    answer = [d[\"answers\"] for d in qs_data[:max_samples]]\n",
    "    \n",
    "    F1 = {'F1': {}}\n",
    "    doc_len = {'doc_len': {}}\n",
    "    None_count = {'none_count': {}}\n",
    "    \n",
    "    answer_types = [\"rb_pred\", \"rl_pred\", \"ext_pred\", \"fil_pred\", \"ext_fil_pred\"]\n",
    "    dict_ = {\"rb_pred\": \"R&B\", \"rl_pred\": \"R&L\", \"ext_pred\": \"Ext\", \"fil_pred\": \"Fil\", \"ext_fil_pred\": \"E&F\"}\n",
    "    \n",
    "    pred_path = Path(pred_dir)\n",
    "    \n",
    "    for answer_type in answer_types:\n",
    "        file_path = pred_path / f\"{answer_type}.json\"\n",
    "        if not file_path.exists():\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            preds = []\n",
    "            lens = []\n",
    "            none_count = 0\n",
    "            \n",
    "            for idx in range(min(max_samples, len(lines))):\n",
    "                try:\n",
    "                    data_line = json.loads(lines[idx])\n",
    "                    pred = data_line.get(answer_type, \"None\")\n",
    "                    preds.append(pred)\n",
    "                    lens.append(data_line.get('input_len', -1) or -1)\n",
    "                except:\n",
    "                    preds.append(\"None\")\n",
    "                    lens.append(-1)\n",
    "                \n",
    "                if pred == \"None\" or pred is None:\n",
    "                    none_count += 1\n",
    "            \n",
    "            if preds:\n",
    "                # Ensure we have matching lengths\n",
    "                eval_answer = answer[:len(preds)]\n",
    "                F1['F1'][dict_[answer_type]] = F1_scorer(preds, eval_answer)\n",
    "                doc_len['doc_len'][dict_[answer_type]] = replace_and_calculate_average(lens)\n",
    "                None_count['none_count'][dict_[answer_type]] = none_count\n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {answer_type}: {e}\")\n",
    "    \n",
    "    print(\"F1 Scores:\")\n",
    "    for k, v in F1['F1'].items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    \n",
    "    print(\"\\nNone Count:\")\n",
    "    for k, v in None_count['none_count'].items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    print(\"\\nAvg Doc Length:\")\n",
    "    for k, v in doc_len['doc_len'].items():\n",
    "        print(f\"  {k}: {v:.1f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    return F1, doc_len, None_count\n",
    "\n",
    "print(\"‚úÖ Evaluation functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcc8fe",
   "metadata": {},
   "source": [
    "## 4. Check Available Log Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0101cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available log directories:\n",
      "\n",
      "üìÅ 200_2_2/\n",
      "   ‚îî‚îÄ‚îÄ hotpotqa/\n",
      "       ‚îî‚îÄ‚îÄ gemini-2.5-flash/\n",
      "           ‚îî‚îÄ‚îÄ prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1/\n",
      "               Predictions: ['rb_pred']\n",
      "           ‚îî‚îÄ‚îÄ prompt_v1_v1_upscaling_chunk_ext1_with_reranking_top_k1_5_top_k2_3_merge_v1_marco_MiniLM_upsampling_4/\n",
      "           ‚îî‚îÄ‚îÄ prompt_v1_v1_upscaling_chunk_ext1_without_reranking_top_k1_5_top_k2_3_merge_v1/\n"
     ]
    }
   ],
   "source": [
    "log_dir = PROJECT_ROOT / 'log'\n",
    "\n",
    "if log_dir.exists():\n",
    "    print(\"Available log directories:\\n\")\n",
    "    for r_path in sorted(log_dir.iterdir()):\n",
    "        if r_path.is_dir() and not r_path.name.startswith('.'):\n",
    "            print(f\"üìÅ {r_path.name}/\")\n",
    "            for dataset in sorted(r_path.iterdir()):\n",
    "                if dataset.is_dir():\n",
    "                    print(f\"   ‚îî‚îÄ‚îÄ {dataset.name}/\")\n",
    "                    for model in sorted(dataset.iterdir()):\n",
    "                        if model.is_dir():\n",
    "                            print(f\"       ‚îî‚îÄ‚îÄ {model.name}/\")\n",
    "                            for version in sorted(model.iterdir()):\n",
    "                                if version.is_dir():\n",
    "                                    # Check what prediction files exist\n",
    "                                    pred_files = list(version.glob(\"*_pred.json\"))\n",
    "                                    pred_names = [f.stem for f in pred_files]\n",
    "                                    print(f\"           ‚îî‚îÄ‚îÄ {version.name}/\")\n",
    "                                    if pred_files:\n",
    "                                        print(f\"               Predictions: {pred_names}\")\n",
    "else:\n",
    "    print(\"No log directory found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3aad74",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation on Gemini Results\n",
    "\n",
    "### 5.1 Configure evaluation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1016afc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: gemini-2.5-flash\n",
      "Index path: 200_2_2\n",
      "Datasets: ['hotpotqa', '2wikimultihopqa', 'musique']\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL = \"gemini-2.5-flash\"\n",
    "R_PATH = \"200_2_2\"  # or \"sum_600_400_raw_1500_500_e5\"\n",
    "DATASETS = [\"hotpotqa\", \"2wikimultihopqa\", \"musique\"]\n",
    "\n",
    "print(f\"Model: {MODEL}\")\n",
    "print(f\"Index path: {R_PATH}\")\n",
    "print(f\"Datasets: {DATASETS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900e698",
   "metadata": {},
   "source": [
    "### 5.2 Find available versions for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e4ef60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hotpotqa:\n",
      "  ‚Ä¢ prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1\n",
      "    Files: ['rb_pred.json']\n",
      "  ‚Ä¢ prompt_v1_v1_upscaling_chunk_ext1_with_reranking_top_k1_5_top_k2_3_merge_v1_marco_MiniLM_upsampling_4\n",
      "  ‚Ä¢ prompt_v1_v1_upscaling_chunk_ext1_without_reranking_top_k1_5_top_k2_3_merge_v1\n",
      "\n",
      "2wikimultihopqa: No results found for gemini-2.5-flash\n",
      "\n",
      "musique: No results found for gemini-2.5-flash\n"
     ]
    }
   ],
   "source": [
    "for dataset in DATASETS:\n",
    "    model_path = log_dir / R_PATH / dataset / MODEL\n",
    "    if model_path.exists():\n",
    "        print(f\"\\n{dataset}:\")\n",
    "        for version in sorted(model_path.iterdir()):\n",
    "            if version.is_dir():\n",
    "                pred_files = list(version.glob(\"*_pred.json\"))\n",
    "                print(f\"  ‚Ä¢ {version.name}\")\n",
    "                if pred_files:\n",
    "                    print(f\"    Files: {[f.name for f in pred_files]}\")\n",
    "    else:\n",
    "        print(f\"\\n{dataset}: No results found for {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6587f72e",
   "metadata": {},
   "source": [
    "### 5.3 Evaluate specific version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74323f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: gemini-2.5-flash / prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1\n",
      "\n",
      "============================================================\n",
      "\n",
      "üìä Dataset: hotpotqa\n",
      "----------------------------------------\n",
      "F1 Scores:\n",
      "  R&B: 8.0100\n",
      "\n",
      "None Count:\n",
      "  R&B: 1\n",
      "\n",
      "Avg Doc Length:\n",
      "  R&B: 648.7\n",
      "\n",
      "==================================================\n",
      "\n",
      "‚ö†Ô∏è 2wikimultihopqa: No results at /home/usman619/python_code/MacRAG/log/200_2_2/2wikimultihopqa/gemini-2.5-flash/prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1\n",
      "\n",
      "‚ö†Ô∏è musique: No results at /home/usman619/python_code/MacRAG/log/200_2_2/musique/gemini-2.5-flash/prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1\n"
     ]
    }
   ],
   "source": [
    "# Set the version to evaluate (copy from above)\n",
    "VERSION = \"prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1\"\n",
    "\n",
    "print(f\"Evaluating: {MODEL} / {VERSION}\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = {}\n",
    "for dataset in DATASETS:\n",
    "    pred_dir = log_dir / R_PATH / dataset / MODEL / VERSION\n",
    "    if pred_dir.exists():\n",
    "        print(f\"\\nüìä Dataset: {dataset}\")\n",
    "        print(\"-\"*40)\n",
    "        results[dataset] = eval_function(dataset, str(pred_dir) + \"/\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è {dataset}: No results at {pred_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8daaa29f",
   "metadata": {},
   "source": [
    "## 6. Run MacRAG Evaluation (if needed)\n",
    "\n",
    "If you don't have results yet, run this command in terminal:\n",
    "\n",
    "```bash\n",
    "cd /home/usman619/python_code/MacRAG\n",
    "\n",
    "OLLAMA_BASE_URL=http://localhost:11434 \\\n",
    "OLLAMA_EMBED_MODEL=nomic-embed-text \\\n",
    "python src/main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --model gemini-2.5-flash \\\n",
    "    --r_path processed/200_2_2 \\\n",
    "    --top_k1 5 --top_k2 3 \\\n",
    "    --prompt_version 1 \\\n",
    "    --with_reranking 0 \\\n",
    "    --chunk_ext 0 \\\n",
    "    --rb  # <-- IMPORTANT: Enable RAG-Base generation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9681ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also run the evaluation from within the notebook\n",
    "# WARNING: This will take a while and make API calls\n",
    "\n",
    "RUN_EVALUATION = False  # Set to True to run\n",
    "\n",
    "if RUN_EVALUATION:\n",
    "    import subprocess\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", str(PROJECT_ROOT / \"src\" / \"main_macrag.py\"),\n",
    "        \"--dataset\", \"hotpotqa\",\n",
    "        \"--model\", \"gemini-2.5-flash\",\n",
    "        \"--r_path\", \"processed/200_2_2\",\n",
    "        \"--top_k1\", \"5\",\n",
    "        \"--top_k2\", \"3\",\n",
    "        \"--prompt_version\", \"1\",\n",
    "        \"--with_reranking\", \"0\",\n",
    "        \"--chunk_ext\", \"0\",\n",
    "        \"--rb\",  # Enable RAG-Base generation\n",
    "    ]\n",
    "    \n",
    "    env = os.environ.copy()\n",
    "    env[\"OLLAMA_BASE_URL\"] = \"http://localhost:11434\"\n",
    "    env[\"OLLAMA_EMBED_MODEL\"] = \"nomic-embed-text\"\n",
    "    \n",
    "    print(f\"Running: {' '.join(cmd)}\")\n",
    "    result = subprocess.run(cmd, env=env, cwd=str(PROJECT_ROOT), capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c61ea0",
   "metadata": {},
   "source": [
    "## 7. Summary Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e8070",
   "metadata": {},
   "source": [
    "## 7. Analysis: Why Scores are Low and How to Improve\n",
    "\n",
    "### Current Issue\n",
    "The current F1 score is low because:\n",
    "1. **Verbose Gemini responses**: Instead of \"Gates v. Collier\", Gemini returns \"The information is not present...\"\n",
    "2. **Using simplified index** (200_2_2) instead of full MacRAG index\n",
    "3. **chunk_ext=0** disables the key MacRAG chunk extension feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47608718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze current predictions\n",
    "import json\n",
    "\n",
    "pred_file = PROJECT_ROOT / 'log' / '200_2_2' / 'hotpotqa' / 'gemini-2.5-flash' / 'prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1' / 'rb_pred.json'\n",
    "\n",
    "if pred_file.exists():\n",
    "    with open(pred_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    verbose_count = 0\n",
    "    none_count = 0\n",
    "    short_answer_count = 0\n",
    "    \n",
    "    print(\"Sample predictions:\\n\")\n",
    "    for i, line in enumerate(lines[:10]):\n",
    "        data = json.loads(line)\n",
    "        pred = data.get('rb_pred', '')\n",
    "        \n",
    "        # Categorize response type\n",
    "        if pred == \"None\" or pred is None:\n",
    "            none_count += 1\n",
    "            cat = \"‚ùå None\"\n",
    "        elif len(pred) > 50 or \"not\" in pred.lower() or \"passage\" in pred.lower():\n",
    "            verbose_count += 1\n",
    "            cat = \"‚ö†Ô∏è Verbose\"\n",
    "        else:\n",
    "            short_answer_count += 1\n",
    "            cat = \"‚úÖ Short\"\n",
    "        \n",
    "        print(f\"{i+1}. [{cat}] Q: {data['question'][:60]}...\")\n",
    "        print(f\"   A: {pred[:80]}{'...' if len(pred) > 80 else ''}\\n\")\n",
    "    \n",
    "    # Count all\n",
    "    for line in lines[10:]:\n",
    "        data = json.loads(line)\n",
    "        pred = data.get('rb_pred', '')\n",
    "        if pred == \"None\" or pred is None:\n",
    "            none_count += 1\n",
    "        elif len(pred) > 50 or \"not\" in pred.lower() or \"passage\" in pred.lower():\n",
    "            verbose_count += 1\n",
    "        else:\n",
    "            short_answer_count += 1\n",
    "    \n",
    "    print(f\"\\nüìä Response Analysis (n={len(lines)}):\")\n",
    "    print(f\"   ‚úÖ Short answers: {short_answer_count} ({100*short_answer_count/len(lines):.1f}%)\")\n",
    "    print(f\"   ‚ö†Ô∏è Verbose responses: {verbose_count} ({100*verbose_count/len(lines):.1f}%)\")\n",
    "    print(f\"   ‚ùå None/Failed: {none_count} ({100*none_count/len(lines):.1f}%)\")\n",
    "else:\n",
    "    print(\"No predictions file found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6143cc",
   "metadata": {},
   "source": [
    "## 8. How to Improve Gemini Results\n",
    "\n",
    "### Option 1: Use Better Prompt (Recommended)\n",
    "Run with `--prompt_version 3` which asks for fewer words:\n",
    "\n",
    "```bash\n",
    "OLLAMA_BASE_URL=http://localhost:11434 \\\n",
    "OLLAMA_EMBED_MODEL=nomic-embed-text \\\n",
    "python src/main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --model gemini-2.5-flash \\\n",
    "    --r_path processed/sum_600_400_raw_1500_500_e5 \\\n",
    "    --top_k1 100 --top_k2 7 \\\n",
    "    --prompt_version 3 \\\n",
    "    --with_reranking 1 \\\n",
    "    --chunk_ext 1 \\\n",
    "    --rb\n",
    "```\n",
    "\n",
    "### Option 2: Post-Process Gemini Responses\n",
    "Extract the actual answer from verbose responses using simple heuristics.\n",
    "\n",
    "### Option 3: Use the Full MacRAG Index\n",
    "Generate proper indices using `gen_index_macrag.py` instead of `gen_index_longrag.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abfe72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-process Gemini responses to extract concise answers\n",
    "def extract_answer(response):\n",
    "    \"\"\"Try to extract a concise answer from verbose Gemini response.\"\"\"\n",
    "    if not response or response == \"None\":\n",
    "        return \"None\"\n",
    "    \n",
    "    # If already short, return as-is\n",
    "    if len(response) < 50 and \"not\" not in response.lower():\n",
    "        return response\n",
    "    \n",
    "    # Common patterns to filter out\n",
    "    skip_patterns = [\n",
    "        \"the provided passages do not\",\n",
    "        \"the information is not\",\n",
    "        \"cannot be determined\",\n",
    "        \"not enough information\",\n",
    "        \"i cannot answer\",\n",
    "        \"based on the passages\",\n",
    "        \"the passages do not\"\n",
    "    ]\n",
    "    \n",
    "    lower_resp = response.lower()\n",
    "    for pattern in skip_patterns:\n",
    "        if pattern in lower_resp:\n",
    "            return \"None\"  # No useful answer\n",
    "    \n",
    "    # Try to extract answer after common prefixes\n",
    "    prefixes = [\"the answer is \", \"answer: \", \"is \"]\n",
    "    for prefix in prefixes:\n",
    "        if prefix in lower_resp:\n",
    "            idx = lower_resp.find(prefix) + len(prefix)\n",
    "            answer = response[idx:].split('.')[0].strip()\n",
    "            if len(answer) < 100:\n",
    "                return answer\n",
    "    \n",
    "    return response  # Return original if no pattern matched\n",
    "\n",
    "# Test extraction\n",
    "test_responses = [\n",
    "    \"Miller v. California\",\n",
    "    \"The provided passages do not contain information about this topic.\",\n",
    "    \"The answer is Gates v. Collier based on the passages.\",\n",
    "    \"None\"\n",
    "]\n",
    "\n",
    "print(\"Testing answer extraction:\\n\")\n",
    "for resp in test_responses:\n",
    "    extracted = extract_answer(resp)\n",
    "    print(f\"Original: {resp[:60]}...\")\n",
    "    print(f\"Extracted: {extracted}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-evaluate with post-processing\n",
    "def eval_with_postprocessing(data, pred_dir, max_samples=200):\n",
    "    \"\"\"Evaluate with post-processed answers.\"\"\"\n",
    "    answer_path = PROJECT_ROOT / \"data\" / \"eval\" / f\"{data}.json\"\n",
    "    \n",
    "    with open(answer_path, encoding='utf-8') as f:\n",
    "        qs_data = json.load(f)\n",
    "    \n",
    "    answer = [d[\"answers\"] for d in qs_data[:max_samples]]\n",
    "    \n",
    "    pred_path = Path(pred_dir)\n",
    "    file_path = pred_path / \"rb_pred.json\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(\"No predictions found\")\n",
    "        return None\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Original predictions\n",
    "    original_preds = []\n",
    "    # Post-processed predictions\n",
    "    processed_preds = []\n",
    "    \n",
    "    for idx in range(min(max_samples, len(lines))):\n",
    "        try:\n",
    "            data_line = json.loads(lines[idx])\n",
    "            pred = data_line.get('rb_pred', \"None\")\n",
    "            original_preds.append(pred)\n",
    "            processed_preds.append(extract_answer(pred))\n",
    "        except:\n",
    "            original_preds.append(\"None\")\n",
    "            processed_preds.append(\"None\")\n",
    "    \n",
    "    eval_answer = answer[:len(original_preds)]\n",
    "    \n",
    "    original_f1 = F1_scorer(original_preds, eval_answer)\n",
    "    processed_f1 = F1_scorer(processed_preds, eval_answer)\n",
    "    \n",
    "    print(f\"üìä F1 Scores for {data}:\")\n",
    "    print(f\"   Original:       {original_f1:.2f}%\")\n",
    "    print(f\"   Post-processed: {processed_f1:.2f}%\")\n",
    "    print(f\"   Improvement:    {processed_f1 - original_f1:+.2f}%\")\n",
    "    \n",
    "    return original_f1, processed_f1\n",
    "\n",
    "# Run post-processed evaluation\n",
    "pred_dir = PROJECT_ROOT / 'log' / '200_2_2' / 'hotpotqa' / 'gemini-2.5-flash' / 'prompt_v1_v1_upscaling_chunk_ext0_without_reranking_top_k1_5_top_k2_3_merge_v1'\n",
    "if pred_dir.exists():\n",
    "    eval_with_postprocessing('hotpotqa', str(pred_dir) + '/')\n",
    "else:\n",
    "    print(\"Run evaluation first to generate predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851812b3",
   "metadata": {},
   "source": [
    "## 9. Root Cause Analysis & How to Match Original Results\n",
    "\n",
    "### üîç Key Finding: Index Size Difference\n",
    "\n",
    "| Index | Chunks | Status |\n",
    "|-------|--------|--------|\n",
    "| `200_2_2` (our test) | **53** | Too small - answers not in corpus |\n",
    "| `200_2_2_e5` (full) | **10,801** | Full index - use this! |\n",
    "| `sum_600_400_raw_1500_500_e5` | ~10k+ | Original MacRAG index |\n",
    "\n",
    "**The low F1 score (3.24%) is primarily because our test index only has 53 chunks, so most answers are simply not retrievable!**\n",
    "\n",
    "### ‚úÖ Solution: Use the Full Pre-built Index\n",
    "\n",
    "Run evaluation with the full `200_2_2_e5` index (which uses `intfloat/multilingual-e5-large` embeddings):\n",
    "\n",
    "```bash\n",
    "cd /home/usman619/python_code/MacRAG\n",
    "\n",
    "# Use the full pre-built index with e5 embeddings\n",
    "python src/main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --model gemini-2.5-flash \\\n",
    "    --r_path processed/200_2_2_e5 \\\n",
    "    --top_k1 100 --top_k2 7 \\\n",
    "    --prompt_version 1 \\\n",
    "    --with_reranking 1 \\\n",
    "    --chunk_ext 0 \\\n",
    "    --rb\n",
    "```\n",
    "\n",
    "**Note**: The pre-built index uses `intfloat/multilingual-e5-large` embeddings, not Ollama. So you don't need to set `OLLAMA_*` env vars when using it.\n",
    "\n",
    "### Expected Performance with Full Index\n",
    "- **R&B (RAG-Base)**: ~35-45% F1\n",
    "- **With chunk extension**: ~45-55% F1\n",
    "- **Original paper (GPT-4)**: ~50-60% F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc4c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Index Size Comparison:\n",
      "\n",
      "   200_2_2 (test): 53 chunks\n",
      "   200_2_2_e5 (full): 10,801 chunks\n",
      "   sum_600_400_e5 (MacRAG): 32,965 chunks\n",
      "\n",
      "‚ö†Ô∏è Our test used only 53 chunks - answers aren't in the corpus!\n",
      "‚úÖ Use 200_2_2_e5 or sum_600_400_e5 for proper evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Compare index sizes\n",
    "import json\n",
    "\n",
    "indices = {\n",
    "    \"200_2_2 (test)\": PROJECT_ROOT / \"data/corpus/processed/200_2_2/hotpotqa/chunks.json\",\n",
    "    \"200_2_2_e5 (full)\": PROJECT_ROOT / \"data/corpus/processed/200_2_2_e5/hotpotqa/chunks.json\",\n",
    "    \"sum_600_400_e5 (MacRAG)\": PROJECT_ROOT / \"data/corpus/processed/sum_600_400_raw_1500_500_e5/hotpotqa/chunks.json\"\n",
    "}\n",
    "\n",
    "print(\"üìä Index Size Comparison:\\n\")\n",
    "for name, path in indices.items():\n",
    "    if path.exists():\n",
    "        with open(path, 'r') as f:\n",
    "            chunks = json.load(f)\n",
    "        print(f\"   {name}: {len(chunks):,} chunks\")\n",
    "    else:\n",
    "        print(f\"   {name}: Not found\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Our test used only 53 chunks - answers aren't in the corpus!\")\n",
    "print(\"‚úÖ Use 200_2_2_e5 or sum_600_400_e5 for proper evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cba1cf0",
   "metadata": {},
   "source": [
    "### Solution: Run with Full Index\n",
    "\n",
    "**To match original paper results, run with the full corpus index:**\n",
    "\n",
    "```bash\n",
    "# Option 1: Use the pre-built 200_2_2_e5 index (10,801 chunks)\n",
    "python main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --r_path processed/200_2_2_e5 \\\n",
    "    --gen_model gemini-2.5-flash \\\n",
    "    --rb --upscaling \\\n",
    "    --top_k1 50 --top_k2 3 \\\n",
    "    --gpu_id 0\n",
    "\n",
    "# Option 2: Use the MacRAG summarized index (32,965 chunks - best quality)\n",
    "python main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --r_path processed/sum_600_400_raw_1500_500_e5 \\\n",
    "    --gen_model gemini-2.5-flash \\\n",
    "    --rb --upscaling \\\n",
    "    --top_k1 50 --top_k2 3 \\\n",
    "    --gpu_id 0\n",
    "```\n",
    "\n",
    "**Key differences from our test:**\n",
    "| Parameter | Test Run | Recommended |\n",
    "|-----------|----------|-------------|\n",
    "| Index chunks | 53 | 10,801+ |\n",
    "| top_k1 | 5 | 50 |\n",
    "| Retrieval quality | ‚ùå | ‚úÖ |\n",
    "\n",
    "**Expected improvement:** F1 from 3% ‚Üí 40-60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76e1c037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Index Embedding Dimensions:\n",
      "\n",
      "   200_2_2 (our test, nomic): 768 dimensions, 53 vectors\n",
      "   200_2_2_e5 (full, e5-base): 1024 dimensions, 10,801 vectors\n",
      "   sum_600_400_e5 (MacRAG): 1024 dimensions, 32,965 vectors\n",
      "\n",
      "‚ö†Ô∏è The full indices use E5 embeddings (1024 dim)\n",
      "‚ö†Ô∏è Our Ollama nomic-embed-text produces 768 dimensions\n",
      "\n",
      "üìù To use full indices, we need to REGENERATE them with Ollama embeddings!\n"
     ]
    }
   ],
   "source": [
    "# Check index dimensions - explains the incompatibility\n",
    "import faiss\n",
    "\n",
    "indices_to_check = {\n",
    "    \"200_2_2 (our test, nomic)\": PROJECT_ROOT / \"data/corpus/processed/200_2_2/hotpotqa/vector.index\",\n",
    "    \"200_2_2_e5 (full, e5-base)\": PROJECT_ROOT / \"data/corpus/processed/200_2_2_e5/hotpotqa/vector.index\",\n",
    "    \"sum_600_400_e5 (MacRAG)\": PROJECT_ROOT / \"data/corpus/processed/sum_600_400_raw_1500_500_e5/hotpotqa/vector.index\"\n",
    "}\n",
    "\n",
    "print(\"üìê Index Embedding Dimensions:\\n\")\n",
    "for name, path in indices_to_check.items():\n",
    "    if path.exists():\n",
    "        idx = faiss.read_index(str(path))\n",
    "        print(f\"   {name}: {idx.d} dimensions, {idx.ntotal:,} vectors\")\n",
    "    else:\n",
    "        print(f\"   {name}: Not found\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è The full indices use E5 embeddings (1024 dim)\")\n",
    "print(\"‚ö†Ô∏è Our Ollama nomic-embed-text produces 768 dimensions\")\n",
    "print(\"\\nüìù To use full indices, we need to REGENERATE them with Ollama embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd85de99",
   "metadata": {},
   "source": [
    "## 10. üéØ Action Plan to Match Original Results\n",
    "\n",
    "### The Problem\n",
    "Our test index only has **53 chunks** from 10 documents, while the full corpus has **10,801+ chunks**. Most questions can't be answered because the relevant information simply isn't in our tiny index.\n",
    "\n",
    "### The Solution: Regenerate Full Index with Ollama Embeddings\n",
    "\n",
    "Since the pre-built indices use E5 embeddings (1024 dim) and we're using Ollama nomic-embed-text (768 dim), we need to regenerate the index.\n",
    "\n",
    "**Step 1: Generate full index with all documents**\n",
    "```bash\n",
    "cd /home/usman619/python_code/MacRAG\n",
    "\n",
    "# Generate index for all hotpotqa documents (not just 10)\n",
    "python src/gen_index_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --output_dir data/corpus/processed/200_2_2_ollama \\\n",
    "    --model nomic-embed-text \\\n",
    "    --chunk_size 200 \\\n",
    "    --overlap 50 \\\n",
    "    --num_docs -1  # Use ALL documents, not just 10\n",
    "```\n",
    "\n",
    "**Step 2: Run evaluation with new full index**\n",
    "```bash\n",
    "python src/main_macrag.py \\\n",
    "    --dataset hotpotqa \\\n",
    "    --r_path processed/200_2_2_ollama \\\n",
    "    --gen_model gemini-2.5-flash \\\n",
    "    --rb --upscaling \\\n",
    "    --top_k1 50 --top_k2 3 \\\n",
    "    --gpu_id 0\n",
    "```\n",
    "\n",
    "### Expected Results\n",
    "| Metric | Current (53 chunks) | Expected (10k+ chunks) |\n",
    "|--------|---------------------|------------------------|\n",
    "| R&B F1 | 3.24% | 40-60% |\n",
    "| R&B EM | ~1% | 25-35% |\n",
    "| Retrieval | ‚ùå Fails | ‚úÖ Works |\n",
    "\n",
    "### Alternative: Use Original E5 Embeddings\n",
    "If you want to use the pre-built indices without regenerating:\n",
    "1. Install sentence-transformers: `pip install sentence-transformers`\n",
    "2. Modify `gemini_handler.py` to use `intfloat/e5-base-v2` for embeddings\n",
    "3. Use the existing `200_2_2_e5` or `sum_600_400_raw_1500_500_e5` indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb729a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üöÄ COMMANDS TO MATCH ORIGINAL PAPER RESULTS\n",
      "======================================================================\n",
      "\n",
      "STEP 1: Generate Full Index with Ollama Embeddings\n",
      "---------------------------------------------------\n",
      "# This will take longer but creates a proper index with all documents\n",
      "\n",
      "cd /home/usman619/python_code/MacRAG\n",
      "\n",
      "python src/gen_index_longrag.py \\\n",
      "    --dataset hotpotqa \\\n",
      "    --chunk_size 200 \\\n",
      "    --min_sentence 2 \\\n",
      "    --overlap 2\n",
      "    # Note: NO --max_docs flag = process ALL documents\n",
      "\n",
      "# Expected: ~5,000+ documents ‚Üí ~10,000+ chunks\n",
      "# Time estimate: 30-60 minutes depending on Ollama speed\n",
      "\n",
      "\n",
      "STEP 2: Run Evaluation with Full Index\n",
      "--------------------------------------\n",
      "python src/main_macrag.py \\\n",
      "    --dataset hotpotqa \\\n",
      "    --r_path processed/200_2_2 \\\n",
      "    --gen_model gemini-2.5-flash \\\n",
      "    --rb --upscaling \\\n",
      "    --top_k1 50 --top_k2 3 \\\n",
      "    --gpu_id 0\n",
      "\n",
      "# Time estimate: ~35 minutes for 200 questions (7s delay per request)\n",
      "# Expected F1: 40-60%\n",
      "\n",
      "\n",
      "ALTERNATIVE: Run in background with nohup\n",
      "-----------------------------------------\n",
      "cd /home/usman619/python_code/MacRAG\n",
      "\n",
      "# Step 1: Index generation\n",
      "nohup python src/gen_index_longrag.py --dataset hotpotqa --chunk_size 200 --min_sentence 2 --overlap 2 > index_log.txt 2>&1 &\n",
      "\n",
      "# Step 2: Evaluation (after index is done)\n",
      "nohup python src/main_macrag.py --dataset hotpotqa --r_path processed/200_2_2 --gen_model gemini-2.5-flash --rb --upscaling --top_k1 50 --top_k2 3 --gpu_id 0 > eval_log.txt 2>&1 &\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the correct commands to run with full corpus\n",
    "print(\"=\" * 70)\n",
    "print(\"üöÄ COMMANDS TO MATCH ORIGINAL PAPER RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "STEP 1: Generate Full Index with Ollama Embeddings\n",
    "---------------------------------------------------\n",
    "# This will take longer but creates a proper index with all documents\n",
    "\n",
    "cd /home/usman619/python_code/MacRAG\n",
    "\n",
    "python src/gen_index_longrag.py \\\\\n",
    "    --dataset hotpotqa \\\\\n",
    "    --chunk_size 200 \\\\\n",
    "    --min_sentence 2 \\\\\n",
    "    --overlap 2\n",
    "    # Note: NO --max_docs flag = process ALL documents\n",
    "\n",
    "# Expected: ~5,000+ documents ‚Üí ~10,000+ chunks\n",
    "# Time estimate: 30-60 minutes depending on Ollama speed\n",
    "\n",
    "\n",
    "STEP 2: Run Evaluation with Full Index\n",
    "--------------------------------------\n",
    "python src/main_macrag.py \\\\\n",
    "    --dataset hotpotqa \\\\\n",
    "    --r_path processed/200_2_2 \\\\\n",
    "    --gen_model gemini-2.5-flash \\\\\n",
    "    --rb --upscaling \\\\\n",
    "    --top_k1 50 --top_k2 3 \\\\\n",
    "    --gpu_id 0\n",
    "\n",
    "# Time estimate: ~35 minutes for 200 questions (7s delay per request)\n",
    "# Expected F1: 40-60%\n",
    "\n",
    "\n",
    "ALTERNATIVE: Run in background with nohup\n",
    "-----------------------------------------\n",
    "cd /home/usman619/python_code/MacRAG\n",
    "\n",
    "# Step 1: Index generation\n",
    "nohup python src/gen_index_longrag.py --dataset hotpotqa --chunk_size 200 --min_sentence 2 --overlap 2 > index_log.txt 2>&1 &\n",
    "\n",
    "# Step 2: Evaluation (after index is done)\n",
    "nohup python src/main_macrag.py --dataset hotpotqa --r_path processed/200_2_2 --gen_model gemini-2.5-flash --rb --upscaling --top_k1 50 --top_k2 3 --gpu_id 0 > eval_log.txt 2>&1 &\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63fd047",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "### Why Current Scores Are Low (3.24% F1)\n",
    "1. **Tiny Index**: Only 53 chunks from 10 documents (vs 10,000+ needed)\n",
    "2. **Embedding Mismatch**: Pre-built indices use E5 (1024 dim), we use nomic (768 dim)\n",
    "3. **Retrieval Failure**: 87% of questions returned \"information not present\"\n",
    "\n",
    "### How to Match Original Results (~50% F1)\n",
    "1. **Regenerate full index** with `gen_index_longrag.py` (remove `--max_docs 10`)\n",
    "2. **Increase top_k1** from 5 to 50 for better retrieval coverage\n",
    "3. **Use Gemini's strengths** - it generates good answers when given relevant context\n",
    "\n",
    "### What's Working ‚úÖ\n",
    "- Gemini API integration with rate limiting\n",
    "- Ollama embeddings (local, free, no rate limits)\n",
    "- FAISS index creation and search\n",
    "- Basic RAG pipeline\n",
    "\n",
    "### Next Steps\n",
    "1. Run `gen_index_longrag.py` without `--max_docs` to build full index\n",
    "2. Run evaluation with `--r_path processed/200_2_2` and `--top_k1 50`\n",
    "3. Compare results with original paper's 50%+ F1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0483f676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Summary Results\n",
      "============================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Method</th>\n",
       "      <th>R&amp;B</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hotpotqa</th>\n",
       "      <td>8.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Method     R&B\n",
       "Dataset       \n",
       "hotpotqa  8.01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if results:\n",
    "    summary_data = []\n",
    "    for dataset, (f1, doc_len, none_count) in results.items():\n",
    "        for method, score in f1['F1'].items():\n",
    "            summary_data.append({\n",
    "                'Dataset': dataset,\n",
    "                'Method': method,\n",
    "                'F1': score,\n",
    "                'Avg Doc Len': doc_len['doc_len'].get(method, 0),\n",
    "                'None Count': none_count['none_count'].get(method, 0)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(summary_data)\n",
    "    print(\"\\nüìä Summary Results\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.pivot_table(index='Dataset', columns='Method', values='F1', aggfunc='first'))\n",
    "else:\n",
    "    print(\"No results to summarize. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c623f9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "608c36d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metric import F1_scorer\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b853e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_and_calculate_average(lst):\n",
    "    # -1ÏùÑ Ï†úÏô∏Ìïú ÎÇòÎ®∏ÏßÄ Í∞íÎì§Ïùò ÌèâÍ∑†ÏùÑ Íµ¨Ìï©ÎãàÎã§.\n",
    "    valid_values = [x for x in lst if x != -1]\n",
    "    if not valid_values:\n",
    "        raise ValueError(\"Î¶¨Ïä§Ìä∏Ïóê Ïú†Ìö®Ìïú Í∞íÏù¥ ÏóÜÏäµÎãàÎã§.\")\n",
    "    \n",
    "    average_of_valid_values = sum(valid_values) / len(valid_values)\n",
    "    \n",
    "    # -1ÏùÑ ÎÇòÎ®∏ÏßÄ Í∞íÎì§Ïùò ÌèâÍ∑†ÏúºÎ°ú ÎåÄÏ≤¥Ìï©ÎãàÎã§.\n",
    "    replaced_list = [average_of_valid_values if x == -1 else x for x in lst]\n",
    "    \n",
    "    # ÏµúÏ¢Ö Î¶¨Ïä§Ìä∏Ïùò ÌèâÍ∑†ÏùÑ Íµ¨Ìï©ÎãàÎã§.\n",
    "    final_average = sum(replaced_list) / len(replaced_list)\n",
    "    \n",
    "    return final_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af414fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(data, pred_dir):\n",
    "    answer_path = \"../data/eval/{}.json\".format(data)\n",
    "    with open(answer_path, encoding='utf-8') as f:\n",
    "            qs_data = json.load(f)\n",
    "    answer = []\n",
    "    for d in qs_data:\n",
    "        answer.append(d[\"answers\"])\n",
    "    F1 = {'F1':{}}\n",
    "    doc_len = {'doc_len':{}}\n",
    "    None_count = {'none_count':{}}\n",
    "    \n",
    "    answer_types = [\"rb_pred\", \"rl_pred\", \"ext_pred\", \"fil_pred\", \"ext_fil_pred\"]\n",
    "    dict_ = {\"rb_pred\":\"R&B\", \"rl_pred\":\"R&L\", \"ext_pred\":\"Ext\", \"fil_pred\":\"Fil\", \"ext_fil_pred\":\"E&F\"}\n",
    "        \n",
    "    for answer_type in answer_types:\n",
    "        try:\n",
    "            with open(pred_dir + answer_type + \".json\", \"r\", encoding = \"utf-8\") as f:\n",
    "                df = f.read()\n",
    "            preds = []\n",
    "            lens = []\n",
    "            none_count = 0\n",
    "            for idx in range(200):\n",
    "                 \n",
    "                try:\n",
    "                    pred = eval(df.split(\"\\n\")[idx])[answer_type]\n",
    "                    preds += [pred]\n",
    "                    lens += [eval(df.split(\"\\n\")[idx])['input_len']]\n",
    "                except:\n",
    "                    pred = eval(df.split(\"\\n\")[idx].replace(\"null\", \"'None'\"))[answer_type]\n",
    "                    preds += [pred]\n",
    "                    lens += [-1]\n",
    "                    \n",
    "                if pred == \"None\":                    \n",
    "                    none_count += 1\n",
    "            \n",
    "            F1['F1'][dict_[answer_type]] = F1_scorer(preds, answer)\n",
    "            doc_len['doc_len'][dict_[answer_type]] = replace_and_calculate_average(lens)\n",
    "            None_count['none_count'][dict_[answer_type]] = none_count\n",
    "        except:\n",
    "            # import pdb;pdb.set_trace()\n",
    "            pass\n",
    "    print(\"F1 ÏÑ±Îä•:\")\n",
    "    print(F1['F1'])\n",
    "    print(\"none Í∞úÏàò:\")\n",
    "    print(None_count['none_count'])\n",
    "    print(\"doc_len:\")\n",
    "    print(doc_len['doc_len'])\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"===============================================\")\n",
    "    # return F1, doc_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed0b12",
   "metadata": {},
   "source": [
    "# 200_2_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee670fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = \"gpt-4o\"\n",
    "model = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "769022ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: gemini-2.5-flash\n",
      "version: base_0_5\n",
      "\n",
      "\n",
      "data: hotpotqa\n",
      "F1 ÏÑ±Îä•:\n",
      "{}\n",
      "none Í∞úÏàò:\n",
      "{}\n",
      "doc_len:\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "===============================================\n",
      "data: 2wikimultihopqa\n",
      "F1 ÏÑ±Îä•:\n",
      "{}\n",
      "none Í∞úÏàò:\n",
      "{}\n",
      "doc_len:\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "===============================================\n",
      "data: musique\n",
      "F1 ÏÑ±Îä•:\n",
      "{}\n",
      "none Í∞úÏàò:\n",
      "{}\n",
      "doc_len:\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "version = \"base_0_5\"\n",
    "print(\"model: {}\".format(model))\n",
    "print(\"version: {}\\n\\n\".format(version))\n",
    "data_list = [\"hotpotqa\", \"2wikimultihopqa\", \"musique\"]\n",
    "for data in data_list:\n",
    "    print(\"data: {}\".format(data))\n",
    "    eval_function(data, f\"./log/200_2_2/{data}/{model}/{version}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288ceeef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
